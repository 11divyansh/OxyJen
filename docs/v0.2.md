# Oxyjen v0.2 Documentation

Oxyjen v0.2 introduces the **first stable execution layer** of the framework.

The focus of this release is correctness, determinism, and clean boundaries:
- Memory-aware graph nodes
- Deterministic retry & fallback execution
- Clear separation between policy and enforcement
- A minimal but stable public LLM API

This document describes **what v0.2 adds**, how the pieces fit together, and how to use them.

---

## Table of Contents
1. [What's New](#whats-new-in-v02)
2. [What's NOT Included](#explicitly-not-included)
3. [Core Concepts](#memory--nodecontext-core-concept)
4. [Public LLM API](#public-llm-api)
5. [LLMNode](#llmnode-graph-primitive)
6. [LLMChain](#llmchain-retry--fallback-execution)
7. [Exception Model](#exception-model-execution-semantics)
8. [OpenAI Transport](#transportopenai)
9. [Stability Guarantees](#stability-guarantees)

---

## What's New in v0.2

### Core additions
- **`Memory`** abstraction with ordered history
- **`NodeContext`** for stateful graph execution
- **`LLMNode`** as the primary graph primitive
- **`LLMChain`** for retry + fallback execution
- **Public `LLM` API** (`of`, `profile`, `chain`)
- **Explicit exception taxonomy** for deterministic retry behavior
- **OpenAI transport layer** (`transport/openai`)

---

## Explicitly NOT Included

The following features are **intentionally deferred** to v0.3+:

### Timeout Enforcement
- Timeout is **policy only** in v0.2
- `LLMChain.builder().timeout(Duration.ofSeconds(5))` sets intent but **does not enforce**
- No timeout exceptions thrown
- No retries triggered by timeout
- **Enforcement planned for v0.3**

### Model Registry
- No centralized model registry in v0.2
- Model validation happens at runtime via OpenAI API
- Unknown models fail on first API call
- **Planned for v0.3**: `Models.register()` and pre-validation

### Conversation Replay
- Memory stores full conversation history
- **BUT**: LLMs are stateless and don't automatically use history
- Each `chat()` call is independent
- Users must manually build conversation context (see [Memory Limitations](#memory-limitations))
- **Planned for v0.3**: Automatic conversation replay via `ChatMemory`

### Additional Features (v0.3+)
- DAG execution
- Concurrency
- Streaming
- Automatic OpenAI smoke tests (manual only)
- Token counting
- Conversation summarization

---

## Memory & NodeContext (Core Concept)

### Memory

`Memory` is a **state container** with two responsibilities:
1. **Key–value storage** for arbitrary data
2. **Ordered history** of events/messages

```java
Memory memory = new InMemoryMemory("chat");

// Key-value storage
memory.put("count", 42);
int count = memory.get("count", Integer.class);

// Ordered history
memory.append("user", "hello");
memory.append("assistant", "hi");

List<MemoryEntry> history = memory.entries();
```

**Key properties:**
- Thread-safe
- Ordered history (insertion order preserved)
- Type-safe retrieval
- History is immutable from the outside

---

### NodeContext

`NodeContext` owns memory across executions.

```java
NodeContext ctx = new NodeContext();

Memory chat = ctx.memory("chat");
Memory system = ctx.memory("system");
```

**Important guarantees:**
- Same memory name → same instance
- Different names → isolated memory
- Memory survives across multiple node executions
- State lives in the context, **not inside nodes**

---

### Memory Limitations (v0.2)

**Critical Understanding:**

**Memory stores history, but LLMs don't automatically use it.**

```java
NodeContext ctx = new NodeContext();
LLMNode node = LLMNode.builder()
    .model("gpt-4o")
    .memory("chat")
    .build();

// Turn 1
String r1 = node.process("My name is Alice", ctx);
// Memory now has: [user: "My name is Alice", assistant: "..."]

// Turn 2
String r2 = node.process("What's my name?", ctx);
// MODEL WON'T REMEMBER! (in v0.2)
// The LLM receives ONLY "What's my name?" as a fresh prompt
```

**Why?**
- Memory **stores** the conversation
- LLMNode **appends** to memory
- But the underlying `ChatModel` is **stateless**
- Each `chat()` call is independent

**Workaround for v0.2:**
Users must manually build conversation context:

```java
Memory memory = ctx.memory("chat");
StringBuilder prompt = new StringBuilder();

for (MemoryEntry entry : memory.entries()) {
    prompt.append(entry.type()).append(": ")
          .append(entry.value()).append("\n");
}

String response = model.chat(prompt.toString());
```

**v0.3 Solution:**
`ChatMemory` will automatically replay conversation history to the LLM.

---

## Public LLM API

### ChatModel (root abstraction)

```java
public interface ChatModel {
    String chat(String input);
}
```

Everything in Oxyjen depends **only** on `ChatModel`.

This allows:
- Real models (OpenAI)
- Fake models (tests)
- Chains (retry + fallback)
- Future providers (Anthropic, local models)

---

### LLM.of(...)

Create a model by name:

```java
ChatModel model = LLM.of("gpt-4o");
String out = model.chat("hello");
```

**Validation:**
- Unknown model → `IllegalArgumentException`
- Null / empty → `IllegalArgumentException`

---

### LLM.profile(...)

Profiles map use-cases to models.

**Default profiles:**
- `fast` → `gpt-4o-mini`
- `cheap` → `gpt-3.5-turbo`
- `smart` → `gpt-4o`

```java
ChatModel model = LLM.profile("fast");
```

**Note:** Profiles are runtime configuration, not execution logic.

---

## LLMNode (Graph Primitive)

`LLMNode` is where Oxyjen differs from LangChain.

**It is:**
- Memory-aware
- Context-driven
- Stateless itself (state lives in `NodeContext`)

```java
LLMNode node = LLMNode.builder()
    .model("gpt-4o")
    .memory("chat")
    .build();

NodeContext ctx = new NodeContext();
String out = node.process("hello", ctx);
```

### What happens internally:

1. User input appended to memory
2. Model invoked (stateless!)
3. Assistant response appended to memory
4. Output returned

### Memory after two runs:

```
user → hello
assistant → echo:hello
user → world
assistant → echo:world
```

**This proves nodes are stateful through context, not internally.** 

---

## LLMChain (Retry + Fallback Execution)

`LLMChain` composes multiple `ChatModel`s for resilience.

```java
ChatModel chain = LLMChain.builder()
    .primary("gpt-4o")
    .fallback("gpt-3.5-turbo")
    .retry(3)
    .build();
```

### Execution guarantees:

1. **Retries happen per model** (3 attempts on `gpt-4o`, then 3 on `gpt-3.5-turbo`)
2. **Fallback occurs after retries are exhausted**
3. **First successful response short-circuits execution**
4. **Final failure throws `LLMException`**

---

### Retry Semantics

Retries happen **only** for transient errors:

| Error Type | Retryable? |
|------------|-----------|
| `RateLimitException` | Yes |
| `NetworkException` | Yes |
| `TimeoutException` (v0.3) | Yes |
| `InvalidAPIKeyException` | No |
| `ModelNotFoundException` | No |
| `TokenLimitExceededException` | No |

---

### Backoff

Two strategies:

1. **Fixed backoff**: 1s between retries
2. **Exponential backoff**: 1s, 2s, 4s, 8s...

```java
LLMChain.builder()
    .primary("gpt-4o")
    .retry(3)
    .exponentialBackoff()   // or fixedBackoff()
    .build();
```

**Note:** Backoff affects retry **delay** only, not execution order.

---

### Timeout (v0.2 - Policy Only)

```java
LLMChain.builder()
    .timeout(Duration.ofSeconds(5))
```

**In v0.2:**
- Timeout is **policy only**
- ❌ No enforcement
- ❌ No exceptions
- ❌ No retries triggered

**Enforcement is planned for v0.3.**

---

## Exception Model (Execution Semantics)

Oxyjen uses **explicit exception types** to drive execution:

| Exception | Meaning | Retry? |
|-----------|---------|--------|
| `InvalidAPIKeyException` | Auth failure | No |
| `ModelNotFoundException` | Model invalid | No |
| `TokenLimitExceededException` | Prompt too large | No |
| `RateLimitException` | Transient quota | Yes |
| `NetworkException` | Server failure | Yes |
| `TimeoutException` (v0.3) | Budget exceeded | Yes |

This makes retry & fallback **deterministic and testable**. 

---

## transport/openai

### Models Registry (limited in v0.2)

```java
Models.isSupported("gpt-4o"); // true
Models.isSupported("foo");    // false
```

- A minimal internal model registry (`Models`) exists
- Used for validation and error classification
- Not user-extensible in v0.2
- No dynamic registration
- **Planned for v0.3**: `Models.register()` and preflight validation

Note: Model metadata is internal in v0.2 and not part of the public API.

---

### OpenAIClient

**Responsible for:**
1. HTTP request construction
2. JSON parsing (minimal, v0.2)
3. Error classification

**Example error mapping:**
- `401` → `InvalidAPIKeyException`
- `429` → `RateLimitException`
- `5xx` → `NetworkException`
- `400` + "context length" → `TokenLimitExceededException`

---

### OpenAIChatModel

Wraps `OpenAIClient` behind `ChatModel`.

```java
ChatModel model = LLM.openai("gpt-4o", apiKey);
String out = model.chat("hello");
```

**This allows OpenAI to plug cleanly into:**
- `LLMNode`
- `LLMChain`
- Future graph executors

---

## Stability Guarantees

For **v0.2.x**:
- Public APIs are frozen
- Behavior is deterministic
- Exception model is stable
- Breaking changes only in v0.3

---

## Quick Start Example

```java
import io.oxyjen.core.*;
import io.oxyjen.llm.*;

public class QuickStart {
    public static void main(String[] args) {
        // 1. Build a resilient chain
        ChatModel chain = LLMChain.builder()
            .primary("gpt-4o")
            .fallback("gpt-4o-mini")
            .retry(3)
            .build();
        
        // 2. Create a graph node
        LLMNode node = LLMNode.builder()
            .model(chain)
            .memory("conversation")
            .build();
        
        // 3. Execute with context
        NodeContext ctx = new NodeContext();
        String response = node.process("Explain quantum computing", ctx);
        
        System.out.println(response);
        
        // 4. Memory persists across calls
        Memory memory = ctx.memory("conversation");
        System.out.println("History size: " + memory.entries().size());
    }
}
```

---

## What's Next?

### v0.3 Roadmap
- Timeout enforcement
- Automatic conversation replay (`ChatMemory`)
- Model registry with pre-validation
- Token counting and management
- Conversation summarization
- Streaming support

---

## Feedback

Found a bug? Have a feature request?

- Open an issue on GitHub
- Join the discussion in Discussions
- Star the repo if you find Oxyjen useful!

---

**Oxyjen v0.2** - Simple. Deterministic. Production-ready graphs for AI.